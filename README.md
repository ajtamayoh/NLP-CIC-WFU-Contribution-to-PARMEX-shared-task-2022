# Using Transformers on Noisy vs. Clean Data for Paraphrase Identification in Mexican Spanish

Authors:

Antonio Tamayo (ajtamayo2019@ipn.cic.mx, ajtamayoh@gmail.com)

Diego A. Burgos (burgosda@wfu.edu)

Alexander Gelbulkh (gelbukh@gelbukh.com)

For bugs or questions related to the code, do not hesitate to contact us (Antonio Tamayo: ajtamayoh@gmail.com)

If you use this code please cite our work: comming soon.

## Abstract

Paraphrase identification is relevant for plagiarism detection, question answering, and machine translation among others. In this work, we report a transfer learning approach using transformers to tackle paraphrase identification on noisy vs. clean data in Spanish as our contribution to the PAR-MEX 2022 shared task. We carried out fine-tuning as well as hyperparameters tuning on BERTIN, a model pre-trained on the Spanish portion of a massive multilingual web corpus. We achieved the best performance in the competition (F1 = 0.94) by fine-tuning BERTIN on noisy data and using it to identify paraphrase on clean data. 

<!--
### How to use our model

Option 1: [Hugging Face Space]()

Option 2: [Hugging Face Model]()
-->

### How to replicate our experiments

[source code](https://github.com/ajtamayoh/NLP-CIC-WFU-Contribution-to-PARMEX-shared-task-2022/blob/main/Code.ipynb)
